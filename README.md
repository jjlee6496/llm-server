# llm-server
 - ë¯¸ë¦¬ ë¹Œë“œëœ llama.cpp ë„ì»¤ì´ë¯¸ì§€ë¥¼ ê°€ì ¸ì™€ì„œ ë¹ ë¥´ê²Œ ëª¨ë¸ ë°°í¬

# ì‚¬ìš©ë°©ë²•
## 1. ëª¨ë¸ ì¤€ë¹„
- ëª¨ë¸ ì¤€ë¹„ í›„ models í´ë”ì— ê°€ì¤‘ì¹˜íŒŒì¼ ë„£ê¸°ê¸°. [EXAONE-3.5-2.4B-Instruct-GGUF(ë¹„ìƒì—…ìš©)](https://huggingface.co/bartowski/EXAONE-3.5-2.4B-Instruct-GGUF)
    - ì˜ˆì‹œë¡œ EXAONE-3.5-2.4B-Instruct-Q2_K.gguf ì‚¬ìš©í•¨
    - [LG EXAONE](https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct)
    - GGUF íŒŒì¼ì´ë€?
- [llama3 í•œêµ­ì–´ ì–‘ìí™”](https://huggingface.co/zoomer75/llama-3.2-Korean-Bllossom-3B-Q4_K_M-GGUF)
- [phi-4(ë§ˆì´í¬ë¡œì†Œí”„íŠ¸ ìµœì‹  ëª¨ë¸)](https://huggingface.co/microsoft/phi-4)
- [í—ˆê¹…í˜ì´ìŠ¤ ê²€ìƒ‰]()
- ëª¨ë¸ ì„ íƒì€ ë¦¬ì†ŒìŠ¤ì— ë”°ë¼, [ë²¤ì¹˜ë§ˆí¬ ì„±ëŠ¥](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?params=0%2C3)ì— ë”°ë¼ ì„ íƒ
- docker-compose.yml ìˆ˜ì •

```yml
command: --model /models/{your_model_file}

```

## 2. ë„ì»¤ ë¹Œë“œ
```bash
docker compose -f docker-compose.yml up

# GPU ì‚¬ìš© ì‹œ
docker compose -f docker-compose-gpu.yml up

```

## 3. ì‚¬ìš©

- localhost:8080 í™”ë©´
![alt text](image.png)
-  Post ìš”ì²­ ë³´ë‚´ê¸° http://localhost:8080/v1/chat/completions
```json
# ìš”ì²­
{
    "messages": [
        {
            "role": "user",
            "content": "ì•ˆë…•"
        }
    ],
    "temperature": 0.7,
    "stream": false
}

# ë‹µë³€
{
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "ì•ˆë…•í•˜ì„¸ìš”! ğŸ˜Š ì–´ë–»ê²Œ ë„ì›€ì´ í•„ìš”í•˜ì‹ ì§€ ìì„¸íˆ ì•Œë ¤ì£¼ì‹œë©´ ë” íš¨ê³¼ì ìœ¼ë¡œ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì„ ê²ƒ ê°™ì•„ìš”. ì—¬í–‰ ê³„íš, ìš”ë¦¬ ë ˆì‹œí”¼ ì°¾ê¸°, ì¼ìƒ ìƒí™œ íŒ ë“± ë‹¤ì–‘í•œ ì£¼ì œì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ì‹ ê°€ìš”? ì¢€ ë” ìì„¸íˆ ì•Œë ¤ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤! ğŸ˜Š",
                "role": "assistant"
            }
        }
    ],
    "created": 1736491685,
    "model": "gpt-3.5-turbo",
    "system_fingerprint": "b4457-ee7136c6",
    "object": "chat.completion",
    "usage": {
        "completion_tokens": 70,
        "prompt_tokens": 13,
        "total_tokens": 83
    },
    "id": "chatcmpl-LzyEZti4WO7NgXoMVnqSFz1xRTl0v2AA",
    "timings": {
        "prompt_n": 13,
        "prompt_ms": 108.351,
        "prompt_per_token_ms": 8.334692307692308,
        "prompt_per_second": 119.98043396000037,
        "predicted_n": 70,
        "predicted_ms": 2624.675,
        "predicted_per_token_ms": 37.495357142857145,
        "predicted_per_second": 26.66996866278682
    }
}
```

# Reference
[llama.cpp ê¹ƒí—ˆë¸Œ](https://github.com/ggerganov/llama.cpp/tree/master)